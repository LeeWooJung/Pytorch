Practice for Attention Mechanism using Pytorch
==============================================

This model(Seq2Seq with various version of attentions) is to translate from English to Korean.  
This model uses tokenizer as follows.
* English tokenizer: spacy
* Korean tokenizer: mecab
  
Version Control
=============================================

* python: 3.6.10
* pytorch: 1.5.1
* torchvision: 0.6.0a0+35d732a
* torchtext: 0.6.0
* numpy: 1.18.5
* konlpy: 0.5.2
* spacy: 2.3.2
