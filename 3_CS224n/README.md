This page records my achivements in the class cs224n.
=============================================================
You can find the assigments & lectures at:  
http://web.stanford.edu/class/cs224n/index.html

Below is the schedule of the class.  

|WEEK|CONTENTS|DONE?|DATE|OUT|DUE
|:---:|:---:|:---:|:---:|:---:|:---:|
|1|Introduction and Word Vectors|YES|2020.08.04|Assignment1||
||Gensim word vectors examples|YES|2020.08.05|||
||Word2Vec Tutorial - The SkipGram Model|YES|2020.08.05|||
||Word2Vec Paper|YES|2020.08.05|||
||Negative Sampling paper|YES|2020.08.09|||
|2|Word Vectors2 and Word Senses|YES|2020.08.10||Assignment1-Done|
||GloVe paper|YES|2020.08.11|||
||Improving Distributional Similarity with Lessons Learned from Word Embeddings|YES|2020.08.12|||
||Evaluation methods for...|YES|2020.08.15|||
|3|Python review session|YES|2020.08.16|||
|4|Word Window Classification, Neural Networks, and Pytorch|YES|2020.08.17|Assignment2|Assignment1|
||Review of differential calculus|YES|2020.08.18|||
|5|Matrix Calculus and Backpropagation|YES|2020.08.19|||
||CS231n notes on network architecture|YES|2020.08.21|||
||CS231n notes on backprop|YES|2020.08.21|||
||Learning Representations by Backprop. Errors|YES|2020.08.21|||
||Derivatives, Backprop., and Vectorization|YES|2020.08.21|||
||Yes you should understand backprop.|YES|2020.08.21||Assignment2-Done|
|6|Linguistic Structure:Dependency Parsing|YES|2020.08.22|Assignment3|Assignment2|
||Incrementality in Deterministic Dependency Parsing|YES|2020.08.22|||
||A Fast and Accurate Dependency Parser using Neural Networks|YES|2020.08.24|||
||Dependency Parsing|YES|2020.08.25||Assignment3-Done|
||Globally Normalized Transition-Based Neural Networks|YES|2020.08.27|||
||Universal Dependencies: A cross-linguistic typology|YES|2020.08.28|||
||Universal Dependencies website|YES|2020.08.28|||
|7|The probability of a sentence? Recurrent Neural Networks and Language Models|YES|2020.08.28|||
||N-gram Language Model|YES|2020.08.28|||
||The Unreasonable Effectiveness of Recurrent Neural Networks|YES|2020.09.02|||
||Sequence Modeling: Recurrent and Recursive Neural Nets(Sec. 10.1 & 10.2)|YES|2020.09.02|||
||On Chomsky and the Two Cultures of Statistical Learning|YES|2020.09.03|||
|8|Vanishing Gradients and Fancy RNNs|YES|2020.09.04|Assignment4|Assignment3|
||Sequence Modeling: Recurrent and Recursive Neural Nets(Sec. 10.3, 10.5, and 10.7-12)|YES|2020.09.08|||
||Learning long-term dependencies with gradient descent is difficult|YES|2020.09.10|||
||On the difficulty of training Recurrent Neural Networks|YES|2020.09.14|||
||Vanishing Gradients Jupyter Notebook|YES|2020.09.18||Assignment4-Done|
||Understanding LSTM Networks|YES|2020.09.18|||
|9|Machine Translation, Seq2Seq and Attention|YES|2020.09.18|||
||Statistical Machine Translation slides, CS224n 2015(lecture2/3/4)|YES|2020.09.21|||
||Statistical Machine Translation (book by Philipp Koehn)|YES|2020.09.21|||
||BLEU (original paper)|YES|2020.09.22|||
||Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)|YES|2020.09.28|||
||Sequence Transduction with Recurrent Neural Networks (early seq2seq speech recognition paper)|YES|2020.09.28|||
||Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)|YES|2020.10.03|||
||Attention and Augmented Recurrent Neural Networks (blog post overview)|YES|2020.10.07|||
||Massive Exploration of Neural Machine Translation Architectures (practical advice for hyperparameter choices)|YES|2020.10.07|||
|10|Practical Tips for Final Projects|YES|2020.10.12|||
||Practical Methodology(Deep Learning book chapter)|YES|2020.10.12|Project Proposal||
|11|Question Answering, the Default Final Project, and an introduction to Transformer architectures|YES|2020.10.12||Assignment4|
||Project Handout|YES|2020.10.14|||
||Attention Is All You Need|YES|2020.10.14|||
||The Illustrated Transformer|YES|2020.10.14|||
||Transformer (Google AI blog post)|YES|2020.10.14|||
||Layer Normalization|YES|2020.10.16|||
||Image Transformer|YES|2020.10.19|||
||Music Transformer: Generating music with long-term structure|YES|2020.10.22|||
|12|ConvNets for NLP|YES|2020.10.26|Assingment5||
||Convolutional Neural Networks for Sentence Classification|YES|2020.10.26|||
||Improving neural networks by preventing co-adaptation of feature detectors|NO||||
||A Convolutional Neural Network for Modelling Sentences|NO||||
|13|Information from parts of words (Subword Models)|NO||||
||Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models|NO||||
||Revisiting Character-Based Neural Machine Translation with Capacity and Compression|NO||||
|14|Contextual Word Representations: BERT (guest lecture by Jacob Devlin)|NO||||
||BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|NO||||
|15|Modeling contexts of use: Contextual Representations and Pretraining. ELMo and BERT.|NO||||
||Contextual Word Representations: A Contextual Introduction.|NO|||Assignment5|
||The Illustrated BERT, ELMo, and co.|NO||||
|16|Reference in Language and Coreference Resolution|NO||||
||Coreference Resolution chapter of Jurafsky and Martin|NO||||
||End-to-end Neural Coreference Resolution|NO||||
|17|Fairness and Inclusion in AI (guest lecture by Vinodkumar Prabhakaran)|NO||||
|18|Constituency Parsing and Tree Recursive Neural Networks|NO||||
||Parsing with Compositional Vector Grammars.|NO||||
||Constituency Parsing with a Self-Attentive Encoder|NO||||
|19|Virtual Office Hours with HuggingFace|NO||||
|20|Recent Advances in Low Resource Machine Translation (guest lecture by Marc'Aurelio Ranzato)|NO||||
|21|Analysis and Interpretability of Neural NLP|NO||||
