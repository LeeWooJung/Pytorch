This page records my achivements in the class cs224n.
=============================================================
You can find the assigments & lectures at:  
http://web.stanford.edu/class/cs224n/index.html

Below is the schedule of the class.  

|WEEK|CONTENTS|DONE?|DATE|OUT|DUE
|:---:|:---:|:---:|:---:|:---:|:---:|
|1|Introduction and Word Vectors|YES|2020.08.04|Assignment1||
||Gensim word vectors examples|YES|2020.08.05|||
||Word2Vec Tutorial - The SkipGram Model|YES|2020.08.05|||
||Word2Vec Paper|YES|2020.08.05|||
||Negative Sampling paper|YES|2020.08.09|||
|2|Word Vectors2 and Word Senses|YES|2020.08.10||Assignment1-Done|
||GloVe paper|YES|2020.08.11|||
||Improving Distributional Similarity with Lessons Learned from Word Embeddings|YES|2020.08.12|||
||Evaluation methods for...|YES|2020.08.15|||
|3|Python review session|YES|2020.08.16|||
|4|Word Window Classification, Neural Networks, and Pytorch|YES|2020.08.17|Assignment2|Assignment1|
||Review of differential calculus|YES|2020.08.18|||
|5|Matrix Calculus and Backpropagation|YES|2020.08.19|||
||CS231n notes on network architecture|YES|2020.08.21|||
||CS231n notes on backprop|YES|2020.08.21|||
||Learning Representations by Backprop. Errors|YES|2020.08.21|||
||Derivatives, Backprop., and Vectorization|YES|2020.08.21|||
||Yes you should understand backprop.|YES|2020.08.21||Assignment2-Done|
|6|Linguistic Structure:Dependency Parsing|YES|2020.08.22|Assignment3|Assignment2|
||Incrementality in Deterministic Dependency Parsing|YES|2020.08.22|||
||A Fast and Accurate Dependency Parser using Neural Networks|YES|2020.08.24|||
||Dependency Parsing|YES|2020.08.24|||
||Globally Normalized Transition-Based Neural Networks|NO||||
||Universal Stanford Dependencies: A cross-linguistic typology|NO||||
||Universal Dependencies website|NO||||
|7|The probability of a sentence? Recurrent Neural Networks and Language Models|NO||||
||N-gram Language Model|NO||||
||The Unreasonable Effectiveness of Recurrent Neural Networks|NO||||
||Sequence Modeling: Recurrent and Recursive Neural Nets(Sec. 10.1 & 10.2)|NO||||
||On Chomsky and the Two Cultures of Statistical Learning|NO||||
|8|Vanishing Gradients and Fancy RNNs|NO||||
||Sequence Modeling: Recurrent and Recursive Neural Nets(Sec. 10.3, 10.5, and 10.7-12)|NO||||
||Learning long-term dependencies with gradient descent is difficult|NO||||
||On the difficulty of training Recurrent Neural Networks|NO||||
||Vanishing Gradients Jupyter Notebook|NO||||
||Understanding LSTM Networks|NO||Assignment4|Assignment2|
|9|Machine Translation, Seq2Seq and Attention|NO||||
||Statistical Machine Translation slides, CS224n 2015(lecture2/3/4)|NO||||
||Statistical Machine Translation (book by Philipp Koehn)|NO||||
||BLEU (original paper)|NO||||
||Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)|NO||||
||Sequence Transduction with Recurrent Neural Networks (early seq2seq speech recognition paper)|NO||||
||Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)|NO||||
||Attention and Augmented Recurrent Neural Networks (blog post overview)|NO||||
||Massive Exploration of Neural Machine Translation Architectures (practical advice for hyperparameter choices)|NO||||
|10|Practical Tips for Final Projects|NO||||
||Practical Methodology(Deep Learning book chapter)|NO||Project Proposal||
|11|Question Answering, the Default Final Project, and an introduction to Transformer architectures|NO|||Assignment4|
||Project Handout|NO||||
||Attention Is All You Need|NO||||
||The Illustrated Transformer|NO||||
||Transformer (Google AI blog post)|NO||||
||Layer Normalization|NO||||
||Image Transformer|NO||||
||Music Transformer: Generating music with long-term structure|NO||||
  
etc...
