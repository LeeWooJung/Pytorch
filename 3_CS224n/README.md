This page records my achivements in the class cs224n.
=============================================================
You can find the assigments & lectures at:  
http://web.stanford.edu/class/cs224n/index.html

Below is the schedule of the class.  

|WEEK|CONTENTS|DONE?|DATE|OUT|DUE
|:---:|:---:|:---:|:---:|:---:|:---:|
|1|Introduction and Word Vectors|YES|2020.08.04|Assignment1||
||Gensim word vectors examples|YES|2020.08.05|||
||Word2Vec Tutorial - The SkipGram Model|YES|2020.08.05|||
||Word2Vec Paper|YES|2020.08.05|||
||Negative Sampling paper|YES|2020.08.09|||
|2|Word Vectors2 and Word Senses|YES|2020.08.10||Assignment1-Done|
||GloVe paper|YES|2020.08.11|||
||Improving Distributional Similarity with Lessons Learned from Word Embeddings|YES|2020.08.12|||
||Evaluation methods for...|YES|2020.08.15|||
|3|Python review session|YES|2020.08.16|||
|4|Word Window Classification, Neural Networks, and Pytorch|YES|2020.08.17|Assignment2|Assignment1|
||Review of differential calculus|YES|2020.08.18|||
|5|Matrix Calculus and Backpropagation|YES|2020.08.19|||
||CS231n notes on network architecture|YES|2020.08.21|||
||CS231n notes on backprop|YES|2020.08.21|||
||Learning Representations by Backprop. Errors|YES|2020.08.21|||
||Derivatives, Backprop., and Vectorization|YES|2020.08.21|||
||Yes you should understand backprop.|YES|2020.08.21||Assignment2-Done|
|6|Linguistic Structure:Dependency Parsing|YES|2020.08.22|Assignment3|Assignment2|
||Incrementality in Deterministic Dependency Parsing|YES|2020.08.22|||
||A Fast and Accurate Dependency Parser using Neural Networks|YES|2020.08.24|||
||Dependency Parsing|YES|2020.08.25||Assignment3-Done|
||Globally Normalized Transition-Based Neural Networks|YES|2020.08.27|||
||Universal Dependencies: A cross-linguistic typology|YES|2020.08.28|||
||Universal Dependencies website|YES|2020.08.28|||
|7|The probability of a sentence? Recurrent Neural Networks and Language Models|YES|2020.08.28|||
||N-gram Language Model|YES|2020.08.28|||
||The Unreasonable Effectiveness of Recurrent Neural Networks|YES|2020.09.02|||
||Sequence Modeling: Recurrent and Recursive Neural Nets(Sec. 10.1 & 10.2)|YES|2020.09.02|||
||On Chomsky and the Two Cultures of Statistical Learning|YES|2020.09.03|||
|8|Vanishing Gradients and Fancy RNNs|YES|2020.09.04|Assignment4|Assignment3|
||Sequence Modeling: Recurrent and Recursive Neural Nets(Sec. 10.3, 10.5, and 10.7-12)|YES|2020.09.08|||
||Learning long-term dependencies with gradient descent is difficult|YES|2020.09.10|||
||On the difficulty of training Recurrent Neural Networks|YES|2020.09.14|||
||Vanishing Gradients Jupyter Notebook|YES|2020.09.18||Assignment4-Done|
||Understanding LSTM Networks|YES|2020.09.18|||
|9|Machine Translation, Seq2Seq and Attention|YES|2020.09.18|||
||Statistical Machine Translation slides, CS224n 2015(lecture2/3/4)|YES|2020.09.21|||
||Statistical Machine Translation (book by Philipp Koehn)|YES|2020.09.21|||
||BLEU (original paper)|YES|2020.09.22|||
||Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)|YES|2020.09.28|||
||Sequence Transduction with Recurrent Neural Networks (early seq2seq speech recognition paper)|YES|2020.09.28|||
||Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)|YES|2020.10.03|||
||Attention and Augmented Recurrent Neural Networks (blog post overview)|YES|2020.10.07|||
||Massive Exploration of Neural Machine Translation Architectures (practical advice for hyperparameter choices)|YES|2020.10.07|||
|10|Practical Tips for Final Projects|YES|2020.10.12|||
||Practical Methodology(Deep Learning book chapter)|YES|2020.10.12|Project Proposal||
|11|Question Answering, the Default Final Project, and an introduction to Transformer architectures|YES|2020.10.12||Assignment4|
||Project Handout|YES|2020.10.14|||
||Attention Is All You Need|YES|2020.10.14|||
||The Illustrated Transformer|NO||||
||Transformer (Google AI blog post)|NO||||
||Layer Normalization|NO||||
||Image Transformer|NO||||
||Music Transformer: Generating music with long-term structure|NO||||
  
etc...
