{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec practice(Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have data for word2vec, you can download the dataset\n",
    "from https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip,  \n",
    "or you can download the dataset using urlib.request like following.\n",
    "\n",
    "### import urlib.request  \n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for preprocessing\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from numpy.random import multinomial\n",
    "\n",
    "# Pakages for training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset  \n",
    "I follow preprocessing .xml file in the following site.  \n",
    "https://wikidocs.net/60855  \n",
    "  \n",
    "1. Load the dataset: open()  \n",
    "2. Extract the contents between CONTENTS and /CONTENTS\n",
    "3. Substitute not text element to ' '\n",
    "4. Split the text into sentences.\n",
    "5. Eiminate the punctuation marks and substitute it to blank\n",
    "   & Change the capital letter to a small letter\n",
    "6. Tokenize the preprocessed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Print one sentence in text:\n",
      "\n",
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "\n",
      "*Print one sentence in sentences:\n",
      "\n",
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new\n",
      "\n",
      "*Print one sentence in pre_sentences:\n",
      "\n",
      "here are two reasons companies fail they only do more of the same or they only do what s new\n",
      "\n",
      "*Print one sentence in tokenized_sentence:\n",
      "\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "\n",
      "Number of tokenized sentences: 242019\n"
     ]
    }
   ],
   "source": [
    "dataset = open('dataset/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "\n",
    "text = '\\n'.join(etree.parse(dataset).xpath('//content/text()'))\n",
    "text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "print(\"*Print one sentence in text:\\n\\n{}\".format(text[:95]))\n",
    "\n",
    "sentences = text.split('.')\n",
    "print(\"\\n*Print one sentence in sentences:\\n\\n{}\".format(sentences[0]))\n",
    "\n",
    "pre_sentences = []\n",
    "for sentence in sentences:\n",
    "    pre_sentences.append(re.sub(r\"[^a-z0-9]+\", \" \", sentence.lower()))\n",
    "\n",
    "print(\"\\n*Print one sentence in pre_sentences:\\n\\n{}\".format(pre_sentences[0]))\n",
    "\n",
    "Tokenized_sentence = [sentence.split(\" \") for sentence in pre_sentences]\n",
    "tokenized_sentence = []\n",
    "for sentence in Tokenized_sentence:\n",
    "    if len(sentence) < 5: continue\n",
    "    tokenized_sentence.append([w for w in sentence if w != ''])\n",
    "print(\"\\n*Print one sentence in tokenized_sentence:\\n\\n{}\".format(tokenized_sentence[0]))\n",
    "\n",
    "print(\"\\nNumber of tokenized sentences: {}\".format(len(tokenized_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample sentences\n",
    "  \n",
    "Since this dataset is very large, we shrink the size of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences: 10000\n"
     ]
    }
   ],
   "source": [
    "nSample = 10000\n",
    "tokenized_sentence = random.sample(tokenized_sentence, nSample)\n",
    "print(\"Number of tokenized sentences: {}\".format(len(tokenized_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to index & Index to Word\n",
    "I follow the instruction from https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb    \n",
    "\n",
    "1. Make vocabulary from tokenized_sentence\n",
    "2. Count the word frequency and cut words that are appear less than min_freq\n",
    "3. Subsampling frequent words\n",
    "4. Create dictionaries for mapping between word and index \n",
    "\n",
    "### Min frequency\n",
    "  \n",
    "Words below the minimun frequency are dropped before training occurs.\n",
    "So, before starting the training, I cut the words that appears less than 'min_freq'\n",
    "\n",
    "### Sub sampling\n",
    "\n",
    "Word2Vec researchers have decided to reduce the amount of learning in a probabilistic way for words that appear frequently in the corpus. This is because there are many opportunities to be updated as much as the frequency of appearance.  \n",
    "Word2Vec researchers say the i-th word (wi)\n",
    "The probability of excluding from learning is defined below.  \n",
    "  \n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$  \n",
    "  \n",
    "They recommend the value of t as 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "for sentence in tokenized_sentence:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = 1\n",
    "        else:\n",
    "            vocabulary[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUT\n",
    "VOCAB = {word:cnt for (word,cnt) in vocabulary.items() if cnt >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub Sampling\n",
    "sum_word_counts = sum(list(VOCAB.values()))\n",
    "words_prob = {word: cnt/float(sum_word_counts) for word, cnt in VOCAB.items()}\n",
    "\n",
    "filtered = []\n",
    "for sentence in tokenized_sentence:\n",
    "    filtered.append([])\n",
    "    for token in sentence:\n",
    "        if token not in VOCAB: continue\n",
    "        prob = 1 - math.sqrt(0.00001/words_prob[token])\n",
    "        if random.random() >= prob:\n",
    "            filtered[-1].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 3134\n",
      "Total number of sentences: 914\n"
     ]
    }
   ],
   "source": [
    "SENTENCE = []\n",
    "\n",
    "for sentence in filtered:\n",
    "    if len(sentence) < 5: continue\n",
    "    SENTENCE.append(sentence)\n",
    "\n",
    "word2index = {word: idx for idx, (word, cnt) in enumerate(VOCAB.items())}\n",
    "index2word = {idx: word for idx, (word, cnt) in enumerate(VOCAB.items())}\n",
    "\n",
    "vocab_size = len(VOCAB)\n",
    "\n",
    "print(\"Total vocabulary size: {}\".format(vocab_size))\n",
    "print(\"Total number of sentences: {}\".format(len(SENTENCE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip gram\n",
    "  \n",
    "We will use Skip gram, not CBOW.  \n",
    "This is the probability distribution for single pair. \n",
    "  \n",
    "$$ P(context|center;\\theta) $$  \n",
    "  \n",
    "Then, maximize this distribution through all word/context pairs.  \n",
    "  \n",
    "$$ max \\prod_{context} \\prod_{center} P(context|center;\\theta) $$  \n",
    "  \n",
    "After then, make this prob. distribution as negative log likelihood  \n",
    "  \n",
    "$$ min_\\theta -\\frac{1}{T} \\Sigma_{center} \\Sigma_{context} log P(context|center;\\theta) $$  \n",
    "  \n",
    "### Define P\n",
    "  \n",
    "We have to define the probability distribution. Assume there are vectors that represent the word in two ways.  \n",
    "1. v : if a word is the center word\n",
    "2. u : if a word is the context word\n",
    "  \n",
    "Then, we can write P as follows:  \n",
    "  \n",
    "$$ P(context|center;\\theta) = \\frac{exp(u^T_{context} v_{center})}{\\Sigma_{w \\in vocab} exp(u^T_{w} v_{center})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pairs of words that exists within the window size\n",
    "\n",
    "1. Negative Sampling\n",
    "2. Get pairs of words that exists within the window size.  \n",
    "  \n",
    "We will use them to train the word2vec embedding model.  \n",
    "Ref.: https://rguigoures.github.io/word2vec_pytorch/\n",
    "  \n",
    "### Negative Sampling\n",
    "  \n",
    "Since, softmax algorithm takes long time cause of large vocabulary, word2vec researcher suggested to use Negative Sampling algorithm.  \n",
    "This algorithm select the words that are not in context words, and use it to calculate simple softmax value.  \n",
    "You can find the paper here: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "  \n",
    "  \n",
    "Select the words that are not in contexts of each word with prob.  \n",
    "$$ P(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\Sigma_{j=0}^{n}f(w_j)^{\\frac{3}{4}}}$$\n",
    "  \n",
    "Since, we applied negative sampling method, the objective function of unsupervised word2vect model changes as follows:  \n",
    "  \n",
    "$$ J_t(\\theta) = log \\sigma (u_o^Tv_c) + \\Sigma_{j ~ P(w)}[log\\sigma(-u_j^Tv_c)]$$  \n",
    "\n",
    "\n",
    "You can change the window size. But we select the value of window size as 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negatvie Sampling\n",
    "sample_prob = {}\n",
    "word_counts = dict(Counter(list(itertools.chain.from_iterable(SENTENCE))))\n",
    "norm_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "for word in word_counts:\n",
    "    sample_prob[word] = word_counts[word]**0.75/norm_factor\n",
    "words = np.array(list(word_counts.keys()))\n",
    "probs = list(sample_prob.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(ng_size, word_pair):\n",
    "    select = []\n",
    "    sample = []\n",
    "    while True:\n",
    "        select = list(multinomial(ng_size, probs))\n",
    "        selected_words = [words[idx] for idx, cnt in enumerate(select) if cnt > 0]\n",
    "        if not(set(selected_words) & set(word_pair)):\n",
    "            break\n",
    "    for idx, cnt in enumerate(select):\n",
    "        for _ in range(cnt):\n",
    "            sample.append(word2index[words[idx]])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "negative_size = 5\n",
    "word_pairs = []\n",
    "\n",
    "# Save word_pairs with negative samples\n",
    "try:\n",
    "    with open('word_pairs.json','r') as f:\n",
    "        word_pairs = json.load(f)\n",
    "except:\n",
    "    for sentence in SENTENCE:\n",
    "        indices = [word2index[word] for word in sentence]\n",
    "\n",
    "        for center_idx in range(len(indices)):\n",
    "            # save window\n",
    "            for context_idx in range(center_idx - window_size, center_idx + window_size + 1):\n",
    "                if context_idx < 0 or context_idx >= len(indices) or context_idx == center_idx: continue\n",
    "                ng_sample = negative_sampling(negative_size, [index2word[indices[center_idx]], index2word[indices[context_idx]]])\n",
    "                word_pairs.append((indices[center_idx], indices[context_idx], ng_sample))\n",
    "    with open('word_pairs.json','w') as f:\n",
    "        json.dump(word_pairs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec model\n",
    "\n",
    "Now, we are ready to train the word2vec model.  \n",
    "\n",
    "1. Get Batch  \n",
    "To speed up word2vec model learning, we use batch learning.  \n",
    "This way makes the training faster and also regularizes the parameters of the model\n",
    "2. Define Model Word2Vec  \n",
    "Initialize the weight of center & context embeddings.  \n",
    "3. Criterion, Optimizer  \n",
    "We use criterion as Cross Entropy Loss.  \n",
    "And use optimizer as Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "emb_dim = 100\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(word_pairs, batch_size = batch_size):\n",
    "    random.shuffle(word_pairs)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for idx, (target, context, negative) in enumerate(word_pairs):\n",
    "        batch_target.append(target)\n",
    "        batch_context.append(context)\n",
    "        batch_negative.append([idx for idx in negative])\n",
    "        if (idx + 1) % batch_size == 0 or idx == len(word_pairs)-1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.emb_target = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb_context = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb_target.weight = nn.Parameter(torch.cat([torch.zeros(1, emb_dim), torch.FloatTensor(vocab_size - 1, emb_dim).uniform_(-0.5 / emb_dim, 0.5 / emb_dim)]))\n",
    "        self.emb_context.weight = nn.Parameter(torch.cat([torch.zeros(1, emb_dim), torch.FloatTensor(vocab_size - 1, emb_dim).uniform_(-0.5 / emb_dim, 0.5 / emb_dim)]))\n",
    "        self.emb_target.weight.requires_grad = True\n",
    "        self.emb_context.weight.requires_grad = True\n",
    "        \n",
    "    def forward(self, target, context, negative):\n",
    "        emb_target = self.emb_target(target)\n",
    "        emb_context = self.emb_context(context)\n",
    "        positive = torch.mul(emb_target, emb_context)\n",
    "        positive = torch.sum(positive, dim=1)\n",
    "        out = torch.sum(F.logsigmoid(positive))\n",
    "        \n",
    "        emb_negative = self.emb_context(negative)\n",
    "        _negative = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        _negative = torch.sum(_negative, dim = 1)\n",
    "        out += torch.sum(F.logsigmoid(-_negative))\n",
    "        return -out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 126.79674339954416\n",
      "Epoch: 2, Loss: 103.63983894796932\n",
      "Epoch: 3, Loss: 98.82628349119405\n",
      "Epoch: 4, Loss: 89.61861005868879\n",
      "Epoch: 5, Loss: 74.51335606030527\n",
      "Epoch: 6, Loss: 57.016483164988585\n",
      "Epoch: 7, Loss: 41.264257170337295\n",
      "Epoch: 8, Loss: 29.11978720628679\n",
      "Epoch: 9, Loss: 20.49834604329304\n",
      "Epoch: 10, Loss: 14.584076025081753\n",
      "Epoch: 11, Loss: 10.553599129086134\n",
      "Epoch: 12, Loss: 7.783419966285204\n",
      "Epoch: 13, Loss: 5.8418169009231775\n",
      "Epoch: 14, Loss: 4.458371350921974\n",
      "Epoch: 15, Loss: 3.450731998908891\n",
      "Epoch: 16, Loss: 2.7037047952104194\n",
      "Epoch: 17, Loss: 2.1405258636573605\n",
      "Epoch: 18, Loss: 1.7112004232035376\n",
      "Epoch: 19, Loss: 1.3785165563792918\n",
      "Epoch: 20, Loss: 1.1175237812591672\n",
      "Epoch: 21, Loss: 0.9117798532994148\n",
      "Epoch: 22, Loss: 0.7475192889210263\n",
      "Epoch: 23, Loss: 0.615669179591753\n",
      "Epoch: 24, Loss: 0.5089675539711355\n",
      "Epoch: 25, Loss: 0.4222492705837253\n",
      "Epoch: 26, Loss: 0.3513898245964496\n",
      "Epoch: 27, Loss: 0.29313624789454945\n",
      "Epoch: 28, Loss: 0.24506533078875095\n",
      "Epoch: 29, Loss: 0.20531937763558952\n",
      "Epoch: 30, Loss: 0.1724117935399902\n",
      "Epoch: 31, Loss: 0.1448905054466947\n",
      "Epoch: 32, Loss: 0.12202898955623584\n",
      "Epoch: 33, Loss: 0.10295424047071217\n",
      "Epoch: 34, Loss: 0.08689463609719977\n",
      "Epoch: 35, Loss: 0.0735381842819671\n",
      "Epoch: 36, Loss: 0.062388257081636506\n",
      "Epoch: 37, Loss: 0.052578691309350174\n",
      "Epoch: 38, Loss: 0.04459613764007611\n",
      "Epoch: 39, Loss: 0.03785332089940982\n",
      "Epoch: 40, Loss: 0.03210985809757945\n",
      "Epoch: 41, Loss: 0.027263152928681315\n",
      "Epoch: 42, Loss: 0.02317888356236077\n",
      "Epoch: 43, Loss: 0.019707511388616167\n",
      "Epoch: 44, Loss: 0.016817690401779533\n",
      "Epoch: 45, Loss: 0.014409772977619641\n",
      "Epoch: 46, Loss: 0.012162506111468926\n",
      "Epoch: 47, Loss: 0.010354768248777643\n",
      "Epoch: 48, Loss: 0.008841011531418995\n",
      "Epoch: 49, Loss: 0.00754920569341952\n",
      "Epoch: 50, Loss: 0.006425910513319565\n",
      "Epoch: 51, Loss: 0.005484233087872938\n",
      "Epoch: 52, Loss: 0.004683257794413703\n",
      "Epoch: 53, Loss: 0.0039967317641696615\n",
      "Epoch: 54, Loss: 0.0034252836277940436\n",
      "Epoch: 55, Loss: 0.002924722724893875\n",
      "Epoch: 56, Loss: 0.0024908374012915166\n",
      "Epoch: 57, Loss: 0.0021337540201910457\n",
      "Epoch: 58, Loss: 0.001821441298945256\n",
      "Epoch: 59, Loss: 0.0015664963691356787\n",
      "Epoch: 60, Loss: 0.0013342859017240547\n",
      "Epoch: 61, Loss: 0.0011397322841636374\n",
      "Epoch: 62, Loss: 0.000977488333833884\n",
      "Epoch: 63, Loss: 0.0008396086515455949\n",
      "Epoch: 64, Loss: 0.0007278353539694288\n",
      "Epoch: 65, Loss: 0.0006124888838105272\n",
      "Epoch: 66, Loss: 0.0005237448171617392\n",
      "Epoch: 67, Loss: 0.00045031918203506527\n",
      "Epoch: 68, Loss: 0.00038463911247559724\n",
      "Epoch: 69, Loss: 0.0003318554389143079\n",
      "Epoch: 70, Loss: 0.00028759664348342734\n",
      "Epoch: 71, Loss: 0.00024230723730715435\n",
      "Epoch: 72, Loss: 0.0002078597947739018\n",
      "Epoch: 73, Loss: 0.00017924021959109845\n",
      "Epoch: 74, Loss: 0.00015522589583081198\n",
      "Epoch: 75, Loss: 0.00013112899632121446\n",
      "Epoch: 76, Loss: 0.00011263062148409328\n",
      "Epoch: 77, Loss: 9.662118135342393e-05\n",
      "Epoch: 78, Loss: 8.357815686489453e-05\n",
      "Epoch: 79, Loss: 7.277362419070377e-05\n",
      "Epoch: 80, Loss: 6.10309325440168e-05\n",
      "Epoch: 81, Loss: 5.23150678084874e-05\n",
      "Epoch: 82, Loss: 4.525982867216387e-05\n",
      "Epoch: 83, Loss: 3.9279106976647173e-05\n",
      "Epoch: 84, Loss: 3.298216371656138e-05\n",
      "Epoch: 85, Loss: 2.823813734079629e-05\n",
      "Epoch: 86, Loss: 2.419615571116366e-05\n",
      "Epoch: 87, Loss: 2.0808374799082962e-05\n",
      "Epoch: 88, Loss: 1.7478378900377847e-05\n",
      "Epoch: 89, Loss: 1.4761317626809904e-05\n",
      "Epoch: 90, Loss: 1.2464992825610456e-05\n",
      "Epoch: 91, Loss: 1.0466486234335204e-05\n",
      "Epoch: 92, Loss: 8.776521295110728e-06\n",
      "Epoch: 93, Loss: 7.3988084335831125e-06\n",
      "Epoch: 94, Loss: 6.141956028388204e-06\n",
      "Epoch: 95, Loss: 5.074848239343193e-06\n",
      "Epoch: 96, Loss: 4.101374708907032e-06\n",
      "Epoch: 97, Loss: 3.332495475149441e-06\n",
      "Epoch: 98, Loss: 2.6184777900645123e-06\n",
      "Epoch: 99, Loss: 2.0937916926192024e-06\n",
      "Epoch: 100, Loss: 1.5695193443318572e-06\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Word2Vec(emb_dim = emb_dim, vocab_size = vocab_size)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    batch_word_pairs = get_batches(word_pairs, batch_size = batch_size)\n",
    "    losses = []\n",
    "    for i in range(len(batch_word_pairs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        tt, ct, nt = batch_word_pairs[i]\n",
    "        loss = model(tt, ct, nt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch,np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_word(word, topn = 5):\n",
    "    word_distance = []\n",
    "    emb = model.emb_target\n",
    "    dist = nn.PairwiseDistance()\n",
    "    idx = word2index[word]\n",
    "    lookup_i = torch.tensor([idx], dtype=torch.long)\n",
    "    v_i = emb(lookup_i)\n",
    "    for j in range(len(VOCAB)):\n",
    "        if j != idx:\n",
    "            lookup_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_j)\n",
    "            word_distance.append((index2word[j], float(dist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('properties', 3.9740302562713623),\n",
       " ('different', 5.2601752281188965),\n",
       " ('older', 5.26859712600708),\n",
       " ('career', 5.358475208282471),\n",
       " ('economic', 5.413461208343506)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_word(\"guy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are few sentences to train the model, it isn't able to learn the word vector properly...  \n",
    "  \n",
    "We remove many sentences because it takes too long time to get negative sample in the corpus but not in context of the center words.  \n",
    "  \n",
    "If we make improvements to speed up to get negative sample, we can get more plausible word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
