{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec practice(Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have data for word2vec, you can download the dataset\n",
    "from https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip,  \n",
    "or you can download the dataset using urlib.request like following.\n",
    "\n",
    "### import urlib.request  \n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for preprocessing\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from numpy.random import multinomial\n",
    "\n",
    "# Pakages for training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset  \n",
    "I follow preprocessing .xml file in the following site.  \n",
    "https://wikidocs.net/60855  \n",
    "  \n",
    "1. Load the dataset: open()  \n",
    "2. Extract the contents between CONTENTS and /CONTENTS\n",
    "3. Substitute not text element to ' '\n",
    "4. Split the text into sentences.\n",
    "5. Eiminate the punctuation marks and substitute it to blank\n",
    "   & Change the capital letter to a small letter\n",
    "6. Tokenize the preprocessed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Print one sentence in text:\n",
      "\n",
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "\n",
      "*Print one sentence in sentences:\n",
      "\n",
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new\n",
      "\n",
      "*Print one sentence in pre_sentences:\n",
      "\n",
      "here are two reasons companies fail they only do more of the same or they only do what s new\n",
      "\n",
      "*Print one sentence in tokenized_sentence:\n",
      "\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n"
     ]
    }
   ],
   "source": [
    "dataset = open('dataset/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "\n",
    "text = '\\n'.join(etree.parse(dataset).xpath('//content/text()'))\n",
    "text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "print(\"*Print one sentence in text:\\n\\n{}\".format(text[:95]))\n",
    "\n",
    "sentences = text.split('.')\n",
    "print(\"\\n*Print one sentence in sentences:\\n\\n{}\".format(sentences[0]))\n",
    "\n",
    "pre_sentences = []\n",
    "for sentence in sentences:\n",
    "    pre_sentences.append(re.sub(r\"[^a-z0-9]+\", \" \", sentence.lower()))\n",
    "\n",
    "print(\"\\n*Print one sentence in pre_sentences:\\n\\n{}\".format(pre_sentences[0]))\n",
    "\n",
    "Tokenized_sentence = [sentence.split(\" \") for sentence in pre_sentences]\n",
    "tokenized_sentence = []\n",
    "for sentence in Tokenized_sentence:\n",
    "    if len(sentence) < 5: continue\n",
    "    tokenized_sentence.append([w for w in sentence if w != ''])\n",
    "print(\"\\n*Print one sentence in tokenized_sentence:\\n\\n{}\".format(tokenized_sentence[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to index & Index to Word\n",
    "I follow the instruction from https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb    \n",
    "\n",
    "1. Make vocabulary from tokenized_sentence\n",
    "2. Count the word frequency and cut words that are appear less than min_freq\n",
    "3. Subsampling frequent words\n",
    "4. Create dictionaries for mapping between word and index \n",
    "\n",
    "### Min frequency\n",
    "  \n",
    "Words below the minimun frequency are dropped before training occurs.\n",
    "So, before starting the training, I cut the words that appears less than 'min_freq'\n",
    "\n",
    "### Sub sampling\n",
    "\n",
    "Word2Vec researchers have decided to reduce the amount of learning in a probabilistic way for words that appear frequently in the corpus. This is because there are many opportunities to be updated as much as the frequency of appearance.  \n",
    "Word2Vec researchers say the i-th word (wi)\n",
    "The probability of excluding ) from learning is defined below.  \n",
    "  \n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$  \n",
    "  \n",
    "They recommend the value of t as 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "for sentence in tokenized_sentence:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = 1\n",
    "        else:\n",
    "            vocabulary[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUT\n",
    "VOCAB = {word:cnt for (word,cnt) in vocabulary.items() if cnt >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub Sampling\n",
    "sum_word_counts = sum(list(VOCAB.values()))\n",
    "words_prob = {word: cnt/float(sum_word_counts) for word, cnt in VOCAB.items()}\n",
    "\n",
    "filtered = []\n",
    "for sentence in tokenized_sentence:\n",
    "    filtered.append([])\n",
    "    for token in sentence:\n",
    "        if token not in VOCAB: continue\n",
    "        prob = 1 - math.sqrt(0.00001/words_prob[token])\n",
    "        if random.random() >= prob:\n",
    "            filtered[-1].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE = []\n",
    "\n",
    "for sentence in filtered:\n",
    "    if len(sentence) < 5: continue\n",
    "    SENTENCE.append(sentence)\n",
    "\n",
    "word2index = {word: idx for idx, (word, cnt) in enumerate(VOCAB.items())}\n",
    "index2word = {idx: word for idx, (word, cnt) in enumerate(VOCAB.items())}\n",
    "\n",
    "vocab_size = len(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pairs of words that exists within the window size\n",
    "\n",
    "1. Negative Sampling\n",
    "2. Get pairs of words that exists within the window size.  \n",
    "We will use them to train the word2vec embedding model.\n",
    "  \n",
    "### Negative Sampling\n",
    "\n",
    "\n",
    "\n",
    "You can change the window size. But we select the value of window size as 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling\n",
    "def sample_negative(sample_size):\n",
    "    sample_prob = {}\n",
    "    \n",
    "    words = sum(filtered, [])\n",
    "    words_counts = {}\n",
    "    for word in words:\n",
    "        if word not in words_count:\n",
    "            words_count[word] = 1\n",
    "        else:\n",
    "            words_count[word] += 1\n",
    "    \n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_prob[word] = word_count[word]**0.75 / normalizing_factor\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_prob.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                word_list.append(words[index])\n",
    "        yield word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "negative_size = 10\n",
    "word_pairs = []\n",
    "\n",
    "negative_samples = sample_negative(10)\n",
    "\n",
    "for sentence in SENTENCE:\n",
    "    indices = [word2index[word] for word in sentence]\n",
    "    \n",
    "    for center_idx in range(len(indices)):\n",
    "        # save window\n",
    "        for context_idx in range(center_idx - window_size, center_idx + window_size + 1):\n",
    "            if context_idx < 0 or context_idx >= len(indices) or context_idx == center_idx: continue\n",
    "            word_pairs.append((indices[center_idx], indices[context_idx], next(negative_samples)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of cutted sentence with window size\n",
    "temp_cnt = 0\n",
    "sentence_len = len(SENTENCE[0])\n",
    "print(SENTENCE[0])\n",
    "\n",
    "for pair in word_pairs:\n",
    "    (center_idx, context_idx) = pair\n",
    "    \n",
    "    print(\"Center: {}, Context: {}\".format(index2word[center_idx], index2word[context_idx]))\n",
    "        \n",
    "    temp_cnt += 1\n",
    "    if temp_cnt == 30: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip gram\n",
    "  \n",
    "We will use Skip gram, not CBOW.  \n",
    "This is the probability distribution for single pair. \n",
    "  \n",
    "$$ P(context|center;\\theta) $$  \n",
    "  \n",
    "Then, maximize this distribution through all word/context pairs.  \n",
    "  \n",
    "$$ max \\prod_{context} \\prod_{center} P(context|center;\\theta) $$  \n",
    "  \n",
    "After then, make this prob. distribution as negative log likelihood  \n",
    "  \n",
    "$$ min_\\theta -\\frac{1}{T} \\Sigma_{center} \\Sigma_{context} log P(context|center;\\theta) $$  \n",
    "  \n",
    "### Define P\n",
    "  \n",
    "We have to define the probability distribution. Assume there are vectors that represent the word in two ways.  \n",
    "1. v : if a word is the center word\n",
    "2. u : if a word is the context word\n",
    "  \n",
    "Then, we can write P as follows:  \n",
    "  \n",
    "$$ P(context|center;\\theta) = \\frac{exp(u^T_{context} v_{center})}{\\Sigma_{w \\in vocab} exp(u^T_{w} v_{center})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec model\n",
    "\n",
    "Now, we are ready to train the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "emb_dim = 50\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_freq, vocab_size, window_size, word_pairs\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.output = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : [vocab_size]\n",
    "        x = self.embedding(x)\n",
    "        # x : [emb_dim]\n",
    "        out = self.output(x)\n",
    "        # out : [vocab_size]\n",
    "        out = out.view(1,-1)\n",
    "        # out : [1, vocab_size]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vocab_size, emb_dim)\n",
    "criterion = F.cross_entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "def get_one_hot(index):\n",
    "    x = torch.zeros(vocab_size).float()\n",
    "    x[index] = 1.0\n",
    "    return x\n",
    "\n",
    "def train(model, data):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for pairs in data:\n",
    "        epoch_loss = 0\n",
    "        x = get_one_hot(pairs[0])\n",
    "        ys = pairs[1:]\n",
    "        \n",
    "        for y in ys:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            y_true = torch.LongTensor([y])\n",
    "            loss = criterion(output, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        total_loss += epoch_loss/len(ys)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = train(model, word_pairs)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Epoch : {0}\\tTime: {1:.4f}s\".format(epoch, end-start))\n",
    "    print(\"Train loss: {:.4f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
