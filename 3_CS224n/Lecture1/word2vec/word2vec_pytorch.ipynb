{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec practice(Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have data for word2vec, you can download the dataset\n",
    "from https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip,  \n",
    "or you can download the dataset using urlib.request like following.\n",
    "\n",
    "### import urlib.request  \n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for preprocessing\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from numpy.random import multinomial\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Pakages for training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset  \n",
    "I follow the steps to preprocess .xml file in the following site.  \n",
    "https://wikidocs.net/60855  \n",
    "  \n",
    "1. Load the dataset: open()  \n",
    "2. Extract the contents between CONTENTS and /CONTENTS\n",
    "3. Using tokenizer(nltk.sent_tokenize), divide the corpus into sentences.\n",
    "4. Eiminate the punctuation marks and change the capital letter to a small letter\n",
    "6. Tokenize the preprocessed sentences using nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Print one sentence in text:\n",
      "\n",
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "\n",
      "*Print one sentence in sentences:\n",
      "\n",
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "\n",
      "*Print one sentence in pre_sentences:\n",
      "\n",
      "here are two reasons companies fail they only do more of the same or they only do what s new \n",
      "\n",
      "*Print one sentence in tokenized_sentence:\n",
      "\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "\n",
      "Number of tokenized sentences: 273424\n"
     ]
    }
   ],
   "source": [
    "dataset = open('dataset/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "\n",
    "text = '\\n'.join(etree.parse(dataset).xpath('//content/text()'))\n",
    "text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "print(\"*Print one sentence in text:\\n\\n{}\".format(text[:95]))\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\n*Print one sentence in sentences:\\n\\n{}\".format(sentences[0]))\n",
    "\n",
    "pre_sentences = []\n",
    "for sentence in sentences:\n",
    "    pre_sentences.append(re.sub(r\"[^a-z0-9]+\", \" \", sentence.lower()))\n",
    "\n",
    "print(\"\\n*Print one sentence in pre_sentences:\\n\\n{}\".format(pre_sentences[0]))\n",
    "\n",
    "tokenized_sentence = [word_tokenize(sentence) for sentence in pre_sentences]\n",
    "\n",
    "print(\"\\n*Print one sentence in tokenized_sentence:\\n\\n{}\".format(tokenized_sentence[0]))\n",
    "\n",
    "print(\"\\nNumber of tokenized sentences: {}\".format(len(tokenized_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to index & Index to Word\n",
    "I follow the code & instructions from https://github.com/theeluwin/pytorch-sgns  \n",
    "\n",
    "  \n",
    "I did preprocessing dataset in the following sequence.\n",
    "1. Count the word that appears in the dataset & save it into word_count variable\n",
    "2. Define idx2word(index to word) and word2idx(word to index) variable\n",
    "3. Make vocabulary from tokenized_sentence\n",
    "4. Define Skipgram function. It returns center word and context word. Context words are padded with 'unk' word.\n",
    "5. Make dataset which are composed of center and context words.\n",
    "6. Define word_frequency variable.\n",
    "7. Define subsampling probability threshold of each word.\n",
    "  \n",
    "Variable:  \n",
    "* t : sub sampling threshold\n",
    "* window_size : window size\n",
    "* num_negs: number of negative words for each center word\n",
    "* max_vocab : usuable word ranking to train the model\n",
    "* emb_dim : how large to make word representation\n",
    "* padding_idx : padding index\n",
    "* n_epochs : number of epochs\n",
    "* batch_size : mini batch size\n",
    "* device : True if current device can use GPU, else False\n",
    "\n",
    "### Skip gram\n",
    "  \n",
    "We will use Skip gram, not CBOW.  \n",
    "The following is the probability distribution for single pair. \n",
    "  \n",
    "$$ P(context|center;\\theta) $$  \n",
    "  \n",
    "Skip gram model maximizes this distribution through all word/context pairs.  \n",
    "  \n",
    "$$ max \\prod_{context} \\prod_{center} P(context|center;\\theta) $$  \n",
    "  \n",
    "After then, make this prob. distribution as negative log likelihood  \n",
    "  \n",
    "$$ min_\\theta -\\frac{1}{T} \\Sigma_{center} \\Sigma_{context} log P(context|center;\\theta) $$  \n",
    "\n",
    "\n",
    "### Sub sampling\n",
    "\n",
    "Word2Vec researchers have decided to reduce the amount of learning in a probabilistic way for words that appear frequently in the corpus. This is because there are many opportunities to be updated as much as the frequency of appearance.  \n",
    "The probability of excluding from learning is defined below.  \n",
    "  \n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$  \n",
    "  \n",
    "But the researchers use the probability like below.  \n",
    "  \n",
    "$$ P(w_i) = \\frac{f(w_i)-t}{f(w_i)} - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "They recommend the value of t as 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.00001 # sub sampling threshold\n",
    "window_size = 5\n",
    "num_negs = 20\n",
    "max_vocab = 20000\n",
    "emb_dim = 300\n",
    "padding_idx = 0\n",
    "n_epochs = 20\n",
    "batch_size = 4096\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = 'unk'\n",
    "word_count = {}\n",
    "word_count[unk] = 1\n",
    "\n",
    "for sentence in tokenized_sentence:\n",
    "    for token in sentence:\n",
    "        if token not in word_count:\n",
    "            word_count[token] = 1\n",
    "        else:\n",
    "            word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = [unk] + [key for key, _ in sorted(word_count.items(), key = (lambda x: x[1]), reverse=True)][:max_vocab-1]\n",
    "word2idx = {idx2word[idx]: idx for idx, _ in enumerate(idx2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20000\n"
     ]
    }
   ],
   "source": [
    "vocab = set([word for word in word2idx])\n",
    "print(\"Vocabulary size: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip gram\n",
    "\n",
    "def skipgram(sentence, index):\n",
    "    left = sentence[max(0, index-window_size):index]\n",
    "    right = sentence[index+1:min(len(sentence), index+window_size) +1]\n",
    "    \n",
    "    return sentence[index], [unk for _ in range(window_size - len(left))] + left + right + [unk for _ in range(window_size - len(right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for sentence in tokenized_sentence:\n",
    "    sent = []\n",
    "    for word in sentence:\n",
    "        sent.append(word if word in vocab else unk)\n",
    "    for idx in range(len(sent)):\n",
    "        center, contexts = skipgram(sent, idx)\n",
    "        train_data.append((word2idx[center], [word2idx[context] for context in contexts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 4475758\n",
      "Randomly chosen index of train_data: 2324351\n",
      "Training data example: (4, [0, 93, 8, 5, 431, 394, 11, 60, 3, 72])\n",
      "The words of example:\n",
      "center: of\n",
      "contexts: unk well in a couple months we had to get "
     ]
    }
   ],
   "source": [
    "print(\"Training data size: {}\".format(len(train_data)))\n",
    "train_example_idx = random.choice(range(0, len(train_data)))\n",
    "\n",
    "print(\"Randomly chosen index of train_data: {}\".format(train_example_idx))\n",
    "print(\"Training data example: {}\".format(train_data[train_example_idx]))\n",
    "print(\"The words of example:\")\n",
    "center, contexts = train_data[train_example_idx]\n",
    "print(\"center: {}\".format(idx2word[center]))\n",
    "print(\"contexts:\", end = \" \")\n",
    "for word in contexts:\n",
    "    print(idx2word[word], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = np.array([word_count[word] for word in idx2word])\n",
    "word_frequency = word_frequency/word_frequency.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub sampling\n",
    "subsample_prob = (word_frequency - t)/(word_frequency) - np.sqrt(t/word_frequency)\n",
    "subsample_prob = np.clip(subsample_prob, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random index: 3231\n",
      "The probability to exclude training the word picking is 0.0\n"
     ]
    }
   ],
   "source": [
    "random_idx = random.choice(range(0, len(list(subsample_prob))))\n",
    "print(\"Random index: {}\".format(random_idx))\n",
    "print(\"The probability to exclude training the word {} is {}\".format(idx2word[random_idx],subsample_prob[random_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "\n",
    "## Word2Vec\n",
    "  \n",
    "Word2Vec class gets maximum vocabulary size(max_vocab), embedding dimension(emb_dim) and padding index as parameters.\n",
    "  \n",
    "This class consists of input layer and output layer.\n",
    "* input layer: it gets center word as Long Tensor  \n",
    "    The weights of this layer is initilazed uniformly ~ U(-0.5/embedding dim, 0.5/embedding dim)\n",
    "* output layer: it gets contexts words and negative words as Long Tensor  \n",
    "    The weights of this layer is initilazed uniformly ~ U(-0.5/embedding dim, 0.5/embedding dim)\n",
    "\n",
    "## Skip gram with Negative Sampling\n",
    "\n",
    "### Negative Sampling\n",
    "  \n",
    "Since, softmax algorithm takes long time cause of large vocabulary, word2vec researcher suggested to use Negative Sampling algorithm.  \n",
    "This algorithm select the words that are not in context words, and use it to calculate simple softmax value.  \n",
    "You can find the paper here: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "  \n",
    "  \n",
    "Select the words that are in vocab with prob(with replacement).  \n",
    "$$ P(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\Sigma_{j=0}^{n}f(w_j)^{\\frac{3}{4}}}$$\n",
    "  \n",
    "Since, we applied negative sampling method, the objective function of unsupervised Word2Vec model changes as follows:  \n",
    "  \n",
    "$$ J_t(\\theta) = log \\sigma (u_o^Tv_c) + \\Sigma_{j ~ P(w)}[log\\sigma(-u_j^Tv_c)]$$  \n",
    "\n",
    "\n",
    "You can change the window size. But we select the value of window size as 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size=max_vocab, emb_dim = emb_dim, padding_idx = 0):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.centers = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx = padding_idx)\n",
    "        self.contexts = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx = padding_idx)\n",
    "        self.centers.weight = nn.Parameter(torch.cat([torch.zeros(1, self.emb_dim),\n",
    "                                                     torch.FloatTensor(self.vocab_size -1, self.emb_dim).uniform_(-0.5/self.emb_dim, 0.5/self.emb_dim)]))\n",
    "        self.contexts.weight = nn.Parameter(torch.cat([torch.zeros(1, self.emb_dim),\n",
    "                                                     torch.FloatTensor(self.vocab_size -1, self.emb_dim).uniform_(-0.5/self.emb_dim, 0.5/self.emb_dim)]))\n",
    "        self.centers.weight.requires_grad = True\n",
    "        self.contexts.weight.requires_grad = True\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.forward_input(data)\n",
    "    \n",
    "    def forward_input(self, data):\n",
    "        vector = torch.LongTensor(data).to(device)\n",
    "        return self.centers(vector)\n",
    "    \n",
    "    def forward_output(self, data):\n",
    "        vector = torch.LongTensor(data).to(device)\n",
    "        return self.contexts(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram with Negative Sampling\n",
    "class SGNS(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_model, vocab_size = max_vocab, num_negs = num_negs, weights = None):\n",
    "        super(SGNS, self).__init__()\n",
    "        self.emb_model = emb_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negs = num_negs\n",
    "        \n",
    "        word_frequency = np.power(weights, 0.75)\n",
    "        word_frequency = word_frequency / word_frequency.sum()\n",
    "        self.weights = torch.FloatTensor(word_frequency)\n",
    "        \n",
    "    def forward(self, center, contexts):\n",
    "        batch_size = center.size()[0]\n",
    "        context_size = contexts.size()[1]\n",
    "        negative = torch.multinomial(self.weights, batch_size * context_size * self.num_negs, replacement = True).view(batch_size, -1)\n",
    "        \n",
    "        centerV = self.emb_model.forward_input(center).unsqueeze(2)\n",
    "        contextsV = self.emb_model.forward_output(contexts)\n",
    "        negativeV = self.emb_model.forward_output(negative).neg()\n",
    "        \n",
    "        context_loss = F.logsigmoid(torch.bmm(contextsV, centerV).squeeze()).mean(1)\n",
    "        negative_loss = F.logsigmoid(torch.bmm(negativeV, centerV).squeeze()).view(-1, context_size, self.num_negs).sum(2).mean(1)\n",
    "        \n",
    "        return -(context_loss + negative_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (centers): Embedding(20000, 300, padding_idx=0)\n",
       "  (contexts): Embedding(20000, 300, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(vocab_size = max_vocab, emb_dim = emb_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGNS(\n",
       "  (emb_model): Word2Vec(\n",
       "    (centers): Embedding(20000, 300, padding_idx=0)\n",
       "    (contexts): Embedding(20000, 300, padding_idx=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgns = SGNS(emb_model = model, vocab_size=max_vocab, num_negs=num_negs, weights=word_frequency)\n",
    "sgns.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimization = torch.optim.Adam(sgns.parameters())\n",
    "print(optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec model\n",
    "  \n",
    "Before training the model, we must sub sample the words.  \n",
    "  \n",
    "### PermutedSubsampledCorpus\n",
    "  \n",
    "Since we have the threshold of subsampling probability of each word, we can simply sample each word according to the probability.  \n",
    "This class returns permuted and sub sampled dataset.\n",
    "  \n",
    "### Train\n",
    "  \n",
    "We used mini-batch training method.  \n",
    "Using DataLoader class, you can split the dataset into mini-batch easily.  \n",
    "To show training process, we use tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "# Now we apply the sub sampling method\n",
    "\n",
    "class PermutedSubsampledCorpus(Dataset):\n",
    "    def __init__(self, train_data = None, subsample_prob = None):\n",
    "        self.data = []\n",
    "        for center, contexts in train_data:\n",
    "            if random.random() > subsample_prob[center]:\n",
    "                self.data.append((center, contexts))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, contexts = self.data[idx]\n",
    "        return center, np.array(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 262/262 [00:35<00:00,  7.34it/s, loss=4.27]\n",
      "[Epoch 2]: 100%|██████████| 262/262 [00:34<00:00,  7.55it/s, loss=4.08]\n",
      "[Epoch 3]: 100%|██████████| 263/263 [00:35<00:00,  7.49it/s, loss=5.42]\n",
      "[Epoch 4]: 100%|██████████| 262/262 [00:36<00:00,  7.08it/s, loss=4.11]\n",
      "[Epoch 5]: 100%|██████████| 262/262 [00:37<00:00,  6.95it/s, loss=4.06]\n",
      "[Epoch 6]: 100%|██████████| 262/262 [00:37<00:00,  7.02it/s, loss=3.97]\n",
      "[Epoch 7]: 100%|██████████| 262/262 [00:36<00:00,  7.17it/s, loss=4]   \n",
      "[Epoch 8]: 100%|██████████| 262/262 [00:35<00:00,  7.37it/s, loss=4.03]\n",
      "[Epoch 9]: 100%|██████████| 262/262 [00:39<00:00,  6.60it/s, loss=3.99]\n",
      "[Epoch 10]: 100%|██████████| 262/262 [00:38<00:00,  6.81it/s, loss=3.98]\n",
      "[Epoch 11]: 100%|██████████| 262/262 [00:36<00:00,  7.16it/s, loss=3.98]\n",
      "[Epoch 12]: 100%|██████████| 262/262 [00:36<00:00,  7.25it/s, loss=3.99]\n",
      "[Epoch 13]: 100%|██████████| 262/262 [00:36<00:00,  7.18it/s, loss=3.97]\n",
      "[Epoch 14]: 100%|██████████| 262/262 [00:35<00:00,  7.37it/s, loss=3.82]\n",
      "[Epoch 15]: 100%|██████████| 262/262 [00:36<00:00,  7.20it/s, loss=3.89]\n",
      "[Epoch 16]: 100%|██████████| 262/262 [00:35<00:00,  7.31it/s, loss=3.89]\n",
      "[Epoch 17]: 100%|██████████| 262/262 [00:35<00:00,  7.42it/s, loss=3.8] \n",
      "[Epoch 18]: 100%|██████████| 262/262 [00:36<00:00,  7.15it/s, loss=3.84]\n",
      "[Epoch 19]: 100%|██████████| 263/263 [00:36<00:00,  7.22it/s, loss=3.81]\n",
      "[Epoch 20]: 100%|██████████| 262/262 [00:37<00:00,  6.99it/s, loss=3.82]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.train()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dataset = PermutedSubsampledCorpus(train_data = train_data, subsample_prob = subsample_prob)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size , shuffle = True)\n",
    "    total_batches = int(np.ceil(len(dataset)/batch_size))\n",
    "    pbar = tqdm(dataloader)\n",
    "    pbar.set_description(\"[Epoch {}]\".format(epoch))\n",
    "    for center, contexts in pbar:\n",
    "        loss = sgns(center, contexts)\n",
    "        optimization.zero_grad()\n",
    "        loss.backward()\n",
    "        optimization.step()\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "idx2vec = model.centers.weight.data.cpu().numpy()\n",
    "pickle.dump(idx2vec, open('idx2vec.dat', 'wb'))\n",
    "torch.save(sgns.state_dict(), 'word2vec.pt')\n",
    "torch.save(optimization.state_dict(), 'word2vec.optim.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get closest word\n",
    "  \n",
    "Using trained model's lookup table, we can find similar word.  \n",
    "If the word's vector representation of model is not good, then model can't predict properly similar word of given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_word(word, topn = 5):\n",
    "    i = word2idx[word]\n",
    "    word_distance = []\n",
    "    dist = nn.PairwiseDistance()\n",
    "    v_i = idx2vec[i]\n",
    "    tensor_i = torch.FloatTensor([v_i])\n",
    "    for j in range(len(vocab)):\n",
    "        if j != i:\n",
    "            v_j = idx2vec[j]\n",
    "            tensor_j = torch.FloatTensor([v_j])\n",
    "            word_distance.append((idx2word[j], float(dist(tensor_i, tensor_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    print(word_distance[:topn])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 1.0328195095062256), ('man', 1.0591298341751099), ('child', 1.2242408990859985), ('mother', 1.265128493309021), ('faiza', 1.293604850769043)]\n"
     ]
    }
   ],
   "source": [
    "closest_word('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 1.0591306686401367), ('studs', 1.1442370414733887), ('max', 1.1473325490951538), ('hale', 1.1486926078796387), ('guy', 1.1503463983535767)]\n"
     ]
    }
   ],
   "source": [
    "closest_word('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('renew', 1.2119112014770508), ('horizons', 1.231445550918579), ('barter', 1.2369675636291504), ('support', 1.2469265460968018), ('certification', 1.2520084381103516)]\n"
     ]
    }
   ],
   "source": [
    "closest_word('free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
